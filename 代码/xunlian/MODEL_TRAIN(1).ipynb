{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee5b2bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = '《乘风破浪的姐姐》成团之夜-1.csv'\n",
    "stop_words_path = 'cn_stopwords.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5767546a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7077787",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:3: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "<timed exec>:3: FutureWarning: The warn_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/3f/hj3gtg8n3gz5xm84jn7ywct40000gn/T/jieba.cache\n",
      "Loading model cost 0.683 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "<timed exec>:70: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7181733849342667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:82: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6803786001072604\n",
      "CPU times: user 37min 34s, sys: 15.3 s, total: 37min 49s\n",
      "Wall time: 38min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "\n",
    "test = pd.read_csv(csv_path,encoding = \"gb18030\",warn_bad_lines=False,error_bad_lines=False)                                     ################ 数据集位置\n",
    "test_data = pd.DataFrame(test)\n",
    "# 打乱数据集\n",
    "re_test_data = test.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "####去除特殊符号并分词####\n",
    "import jieba\n",
    "import re\n",
    "# 使用jieba进行分词\n",
    "def chinese_word_cut(mytext):\n",
    "    # 去除[@用户]避免影响后期预测精度  \n",
    "    mytext = re.sub(r'@\\w+','',mytext)\n",
    "    # 去除数字字母的字符串\n",
    "    mytext = re.sub(r'[a-zA-Z0-9]','',mytext)\n",
    "    return \" \".join(jieba.cut(mytext))\n",
    "# apply的方法是将数据着行处理\n",
    "re_test_data['cut_review'] = re_test_data.全文内容.apply(chinese_word_cut)          ################# 数据集内容列名字\n",
    "\n",
    "####停用词处理####\n",
    "import re\n",
    "# 获取停用词列表\n",
    "def get_custom_stopwords(stop_words_file):\n",
    "    with open(stop_words_file,encoding='utf-8') as f:                         \n",
    "        stopwords = f.read()\n",
    "    stopwords_list = stopwords.split('\\n')\n",
    "    custom_stopwords_list = [i for i in stopwords_list]\n",
    "    return custom_stopwords_list\n",
    "cachedStopWords = get_custom_stopwords(stop_words_path)                              #################停用词文件位置\n",
    "# 去除停用词方法\n",
    "def remove_stropwords(mytext):\n",
    "    return \" \".join([word for word in mytext.split() if word not in cachedStopWords])\n",
    "re_test_data['remove_strop_word'] = re_test_data.cut_review.apply(remove_stropwords)\n",
    "\n",
    "####保存数据####\n",
    "# 截取处理后的评论数据和标签\n",
    "re_data = re_test_data.loc[:,['remove_strop_word','微博情绪']]                             ################### 数据集标签列名字\n",
    "# 将数据保存为新的csv\n",
    "re_data.to_csv (\"re_sentiment_data.csv\" , encoding = \"utf_8_sig\")\n",
    "\n",
    "####数据分割####\n",
    "X = re_test_data['remove_strop_word']\n",
    "y = re_test_data.微博情绪\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=11)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 加载模型及保存模型\n",
    "import joblib\n",
    "# 朴素贝叶斯算法\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# TFIDF模型\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# 管道模型可将两个算法进行连接\n",
    "from sklearn.pipeline import Pipeline\n",
    "# SVM\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "####使用SVM进行训练####\n",
    "TFIDF_SVM_Sentiment_Model = Pipeline([\n",
    "    ('TFIDF', TfidfVectorizer()),\n",
    "    ('SVM', SVC(C=0.95,kernel=\"linear\",probability=True))\n",
    "])\n",
    "TFIDF_SVM_Sentiment_Model.fit(X_train[:30000],y_train[:30000])\n",
    "svm_test_score = TFIDF_SVM_Sentiment_Model.score(X_test,y_test)\n",
    "joblib.dump(TFIDF_SVM_Sentiment_Model, 'tfidf_svm1_sentiment.model')\n",
    "print(svm_test_score)\n",
    "\n",
    "\n",
    "####使用NBM进行训练\n",
    "TFIDF_NB_Sentiment_Model = Pipeline([\n",
    "    ('TFIDF', TfidfVectorizer()),\n",
    "    ('NB', MultinomialNB())\n",
    "])\n",
    "# 取数据进行训练\n",
    "nbm = TFIDF_NB_Sentiment_Model.fit(X_train[:80000],y_train[:80000])\n",
    "nb_train_score = TFIDF_NB_Sentiment_Model.score(X_test,y_test)\n",
    "joblib.dump(TFIDF_NB_Sentiment_Model, 'tfidf_nb_sentiment.model')\n",
    "print(nb_train_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41af9f1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5c2de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### 使用模型进行情感打分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "363ad2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "真 漂亮 ， 云南 确实 是 个 好 地方 啊 \n",
      "\n",
      "积极情绪，概率：0.9711806327772461\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import joblib\n",
    "# 获取停用词列表\n",
    "def get_custom_stopwords(stop_words_file):\n",
    "    with open(stop_words_file,encoding='utf-8') as f:\n",
    "        stopwords = f.read()\n",
    "    stopwords_list = stopwords.split('\\n')\n",
    "    custom_stopwords_list = [i for i in stopwords_list]\n",
    "    return custom_stopwords_list\n",
    " \n",
    " \n",
    "# 去除停用词方法\n",
    "def remove_stropwords(mytext,cachedStopWords):\n",
    "    return \" \".join([word for word in mytext.split() if word not in cachedStopWords])\n",
    " \n",
    "# 处理否定词不的句子\n",
    "def  Jieba_Intensify(text):\n",
    "    word = re.search(r\"不[\\u4e00-\\u9fa5 ]\",text)\n",
    "    if word!=None:\n",
    "        text = re.sub(r\"(不 )|(不[\\u4e00-\\u9fa5]{1} )\",word[0].strip(),text)\n",
    "    return text\n",
    " \n",
    "# 判断句子消极还是积极\n",
    "def IsPoOrNeg(text):\n",
    "    # 加载训练好的模型     \n",
    "#     model = joblib.load('tfidf_nb_sentiment.model')\n",
    "    model = joblib.load('tfidf_svm1_sentiment.model')\n",
    "    # 获取停用词列表   \n",
    "    cachedStopWords = get_custom_stopwords(stop_words_path)\n",
    "    # 去除停用词    \n",
    "    text = remove_stropwords(text,cachedStopWords)\n",
    "    # jieba分词         \n",
    "    seg_list = jieba.cut(text, cut_all=False)\n",
    "    text = \" \".join(seg_list)\n",
    "    # 否定不处理\n",
    "    text = Jieba_Intensify(text)\n",
    "    y_pre =model.predict([text])\n",
    "    proba = model.predict_proba([text])[0]\n",
    "    if y_pre[0]==1:\n",
    "        print(text,\"\\n\\n积极情绪，概率：\"+str(proba[1]))\n",
    "    else:\n",
    "        print(text,\"\\n\\n消极情绪,概率：\"+str(proba[0]))\n",
    " \n",
    "IsPoOrNeg(\"真漂亮，云南确实是个好地方啊\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
